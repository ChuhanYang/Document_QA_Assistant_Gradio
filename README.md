# ğŸ“„ Local Document Q&A Assistant with LLM

This is a lightweight, privacy-first LLM-powered application that allows you to upload a document (PDF, TXT, or DOCX), preview its contents, and ask questions about it â€” all **fully offline** using open-source tools.

> ğŸ’¡ Powered by a local language model (`gemm2a:2b` via [Ollama](https://ollama.com)) and fast embeddings (`MiniLM`) â€” no API keys, no internet required after setup.

---

## âœ¨ Features

- âœ… Upload documents in **PDF, TXT, or DOCX**
- ğŸ§  Extract key content and **ask natural language questions**
- ğŸ“¸ Preview first page of PDF or snippet of text
- ğŸ—‚ï¸ Multi-format support (UTF-8 and non-standard encodings)
- ğŸ’¬ Interactive **chat history**
- ğŸ“ Downloadable Q&A transcript

---

## ğŸš€ Demo

![Demo Screenshot](https://github.com/ChuhanYang/Document_QA_Assistant_Gradio/blob/756653d4823fd022ae97b8b0dfaa28d2d0afe64e/demo_preview.png)  
> *Chatting with a article and getting an instant summary + Q&A.*

---

## ğŸ›  Installation

### 1. Clone the repository

```bash
git clone https://github.com/your-username/local-doc-qa.git
cd local-doc-qa
```

### 2. Set up a virtual environment (recommended)

```bash
python -m venv .venv
source .venv/bin/activate  # or .venv\\Scripts\\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Install and run Ollama

Download [Ollama](https://ollama.com/download), then pull a lightweight local model:

```bash
ollama pull gemma2:2b
```

> â„¹ï¸ You can also use `llama2` or `mistral` if your system has enough RAM (â‰¥7 GB).

---

## âœ… Run the App

```bash
python doc_qa_app.py
```

Then open the local Gradio interface (usually at `http://127.0.0.1:7860`).

---

## ğŸ§ª Requirements

- Python 3.9+
- 4 GB+ RAM (for `gemma:2b`)
- Ollama (local LLM runtime)

---

## ğŸ“‚ File Structure

```
â”œâ”€â”€ doc_qa_app.py          # Main app
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This file
â””â”€â”€ demo.png               # (Optional) UI screenshot
```

---

## âš™ï¸ Powered By

- [Gradio](https://gradio.app) â€“ UI for ML apps
- [LlamaIndex](https://github.com/jerryjliu/llama_index) â€“ Document indexing and vector search
- [Ollama](https://ollama.com) â€“ Local model hosting (Mistral, Llama2, Gemma)
- [MiniLM](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) â€“ Fast sentence embeddings

---

## ğŸ§  Future Improvements

- ğŸ”„ Support multiple file uploads and cross-doc search
- ğŸ” Document redaction (PII filtering)
- ğŸ“± Mobile-friendly interface
- ğŸŒ Hugging Face or Docker deployment

---

## ğŸ™‹â€â™‚ï¸ Author

**Chuhan Yang**  
Data Scientist Â· ML Enthusiast Â· Public Transit Analytics  
[LinkedIn](https://www.linkedin.com/in/chuhan-yang/) â€¢ [GitHub](https://github.com/ChuhanYang)


